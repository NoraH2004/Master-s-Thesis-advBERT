%TC:incbib
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:group table 0 1
%TC:group tabular 1 1

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------Introduction------------------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Introduction}

%With the introduction of transformer models using attention layers, LSTM networks and RNNs are not necessary anymore. The Transformer is a model language architecture that aims to solve Seq2Seq tasks while handling long term dependencies. 

\todo{Neu schreiben}
With the integration of computer vision for self-driving cars, natural language processing for controlling our smart homes as well as other applications of machine learning in our daily lives, our reliance on technology increases and, with it, also the concerns about security of machine learning. These security concerns grow as natural language processing models are deployed in production systems such as fake news detectors and home assistants. Adversarial examples are small, and often imperceptible perturbations applied to the input data in an effort to fool a deep classifier to incorrect classification.
Adversarial examples are a way to highlight model vulnerabilities and are useful for evaluation and interpretation. 
In my Master's Thesis I want to generate adversarial text to attack a model based on the Bidirectional Encoder Representations from Transformers (BERT) used for Aspect-Based Sentiment Analysis (ABSA), a task in Natural Language Processing (NLP). NLP is a field of artificial intelligence (AI) that addresses the interaction between computers and humans using natural language. In 2018 Google achieved a breakthrough in NLP by introducing BERT, a powerful neural network architecture that leverages transfer-learning by pre-training the model on a large corpora with subsequent fine-tuning on a variety of downstream-tasks like text-classification, question answering, sentiment analysis and more. 
Given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection it is highly concerning that the security vulnerabilities of Deep Learning models in NLP are still largely unknown and not a large field of research.

\textbf{RQ1}: Is there a research question?

%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-------------------Theoretical Background--------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Theoretical Background and Related Work}

In this section, the state of the art research related to the respective research area is outlined. First, the theoretical concepts of Deep Neural Networks (DNN), NLP, and adversarial examples are recalled. Secondly, we provide an overview of the proposed attacking methods on NLP to date. These concepts, combined with the review of related work is used as a basis for the experiments conducted in the practical part of this paper. 

%-------------------------------DNN---------------------------------
\subsection{Deep Learning}

Deep neural networks are neural networks (NN) inspired by the biological neural network of a human brain to learn from examples and build knowledge. 
DNNs perform a  non-linear data transformation and can be used for multiple purposes, e.g., image-recognition, text-classification, audio-transcription, and more \cite{rawat2017deep}.
The input of a DNN consists of multiple data points that are transformed into a meaningful output by the model. The key concept is that the DNN learns the data distribution by means of self found features.
In the training process of a Deep Neural Network, a model is repeatedly fed with samples of a training-dataset $x \in X$ and the respective expected output $y$. \todo{explain data transformations - neurons - weights - feed-forward} The model improves by consciously adjusting the model in the process of transforming the input $x$ to the output $y$.  This process runs until the mapping from input to output no longer improves. 

\todo{In more detail!}
The accuracy of the mapping during training is calculated through the loss function, which measures the distance between expected output $y$ and predicted output $\hat{y}$.
The accuracy describes the percentage of correct matches between $y$ and $\hat{y}$ and is used as a benchmark for model performance related to a set of samples. 
A problem that can occur while training DNNs is the lack of generalization towards unknown samples when a model "overfits" to the training data.
Therefore, the accuracy then has to be calculated on a second test-dataset, which is disjunctive with the dataset used for training.

DNN models contain intermediate, so-called hidden layers, which perform non-linear transformations to the inputs. Each layer forwards data point matrices and can adjust the values and dimensions in the matrix. Every layer consist of neurons with associated weights and biases. A neuron receives a data point forwarded from a previous layer, multiplies it with the associated weight, and adds the bias. The resulting value is passed on to an activation function, which creates a new activation value that is forwarded to the connected neuron of the following layer until the output layer is reached.

Figure xy shows a representation of a deep neural network. The inputs $x_1, \dots, x_n$ are forwarded to the connected layers, where the activation function calculates the neuron's respective activation value using the weight and the bias.
%todo explain backprpagation
\todo{explain backproagation}
The backpropagation mechanism optimizes the weight of each neuron during training by considering a batch of samples with their expected outputs. It computes the gradient of the loss function with respect to the weights and adjusts them accordingly. 

After the training process is complete, meaning the model does not improve anymore, it is used in inference mode, where new and unseen input data can be transformed without a known output. Weights are no longer adjusted.
Over the last 30 years, multiple different model architectures were developed for different use-cases, varying in number, structure, and functionality of hidden layers. Design choices are made considering  (amongst others) input data type, expected results, and computational resources \cite{buduma2017fundamentals}.

In recent years DNNs have shown to achieve great success in many use-cases, such as classification tasks, image- and text generation and decision making systems. Their strengths have led to the broad deployment of systems for problems in the physical world. 
\todo{source}

%-------------------------------------------------------------------
%-------------------------------NLP---------------------------------
%-------------------------------------------------------------------

\subsection{Natural Language Processing}
    \label{sec:natural_language_processing}
    
In NLP (also called Computational Linguistics), researchers aim to explore the interaction between computers and natural language. Natural language generating systems transform machine-readable information into natural language. In contrast, natural language understanding systems aim to convert human language into formal, machine-readable representations such as parse trees or first-order logic. Most NLP tasks apply to both generating and understanding human language \cite{kumar2011natural}.

The research field has emerged as an interdisciplinary area. Fundamental applications of NLP include automatic summarization, machine translation (MT), language identification, Part-of-Speech tagging, parsing, language modelling, textual entailment\footnote{A binary prediction of a relation between two text fragments. E.g., Peter is snoring - A man sleeps.}, and sentiment analysis (SA) \cite{mani2001automatic, dostert1955georgetown, tang2017phonetic,  schmid1994part, bengio2003neural, socher2013recursive,dagan2005pascal, prabowo2009sentiment}. DNNs-based text classification continuously gains importance in information understanding and analysis, as, e.g., many recommendation systems rely on it, as well as for enhancing the safety in online discussion environments, e.g., toxic content detection, and many more \cite{kumar2011natural}. 



The following chapter will give a brief overview, reviewing the most important milestones in NLP, covering both NLP with and without the use of deep learning.

\subsubsection{The History of NLP}
    \label{sec:the_history_of_NLP}
The origins of NLP can be traced back to the 1960s \cite{dostert1955georgetown, Hutchins2006TheFP}\footnote{A report about the research from this time, compiled from reports and newspaper articles.}. With the introduction of DNNs, the objective of automated NLP was established. One of the first NLP tasks include machine translation (MT), a field of research that deals with translating pieces of text from one human language to another. These tasks were solved using simplistic approaches and are based on dictionary lookup and word-by-word substitution. 
The first results in MT were disappointing as structural and lexical ambiguity of language was ignored. An example is the machine translation system developed by scientists at Georgetown University and IBM in 1955 \cite{dostert1955georgetown}, who translated the English sentence \textit{"The spirit is willing but, the flesh is weak"} into Russian and then back to English, receiving the result sentence \textit{"The vodka is good, but the meat is rotten"}.
 In the 1970s, more language processing projects emerged, one of which called ELISA, a system introduced by the computer scientist Weizenbaum that replicated the conversation of a psychologist with a  \cite{weizenbaum1966eliza}. Although it had no understanding of conversation, it gave the appearance of intelligence by permuting the user input and using stock phrases and standard patterns. In the 1970s, computer scientists and linguistics realized that isolated solutions to natural language problems were not effective, and they started to focus on incorporating the respective fields of science in order to bridge the gap between computer science and linguistics \cite{kumar2011natural}. 
 Research in the 1970s conjectured that the problem of competent automatic translation of language is equivalent to the full understanding of text \cite{carbonell1981steps}. The tremendous amount and complexity of information required for language understanding presented significant problems.  Different approaches were used to address those challenges, e.g., tokenizing text documents (segmenting text into meaningless words and punctuation). Word representations were created manually with complex hard-coded rules to assign meaning to these tokens, used to perform an in-depth analysis of linguistic \cite{catania1972chomsky}. 
 More research was carried out, and computers with faster CPUs and cheaper memory led to the first noteworthy results in Natural Language Processing.  With statistical models coming as a revolution in the field \cite{bahl1989tree}, complex, hard-coded systems were replaced. 
 These novel models overcame the complexity barrier of the handwritten, rule-based word representation models through automatic learning. The success of these models in soft, probabilistic decisions led to an increased scientific focus on the approach of word embeddings.
 
 
\textbf{Word Embeddings}
    \label{sec:word_embeddings}

Text consists of small units like words and characters that do not make sense to a computer program. Therefore, word embeddings represent textual input numerically, so that computer programs can process it. They are fixed-length, dense, real-valued vector representations of words.
There exist different methods to generate word embeddings. One of the first proposed methods is one-hot encoding, a word-to-integer mapping. Given a vocabulary containing $N$ words, each word is assigned an integer index $i \in {1, \dots N}$. 
This method results in $N$-dimensional sparse vectors of mostly zeros, except one entry at the position of the corresponding word it represents. Though, this method has two obvious disadvantages. Namely, the vector size increases with the size of the vocabulary. If $N$ increases, a higher amount of data, as well as computer power, is needed for processing. The second downside is that with this method, it is hardly possible to capture linguistic relationships between words. E.g. the vector representation of the word $cat$ is closer to the one of the word $car$ than to the word $dog$, even though the relationship between the words $cat$ and $dog$ seems to should be closer \cite{rodriguez2018beyond}.
Word embeddings have become an integral part of NLP since it was shown that the performance of models for a wide range of different downstream tasks\footnote{A downstream task is a supervised-learning task that utilizes a pre-trained language model \cite{alammar2018illustrated}.} could be enhanced by using pre-trained embeddings as initialization \cite{mikolov2013efficient, pennington2014glove}. 
For that reason, more advanced word embedding methods were introduced, based on word count and prediction. Count based models work with the number of times certain words occur together in a text. These models give deep insights into statistical numbers but have shown to perform poorly in most NLP tasks \cite{neubig2016generalizing}.

 
\textbf{DNN-based Language Models}
    \label{sec:DNN-based_Language_models}
    
Around 2000, Bengio et al. first introduced prediction-based, neuronal network language models \cite{bengio2003neural}. A language model is a probability distribution of sequences of tokens in a language. For every given sequence of tokens, the model determines the probability of that sequence to occur in the given language. Moreover, the language modeling task aims to predict the next word of a sentence, given the previous words. The difficulty here lies in the \textit{curse of dimensionality} of natural language: a sequence of words used to test the model is most likely different from the sequences used in training.  Bengio et al.'s neural network language model consisted of a one-hidden layer feed-forward neural network and word feature vector embedding.
They used a vector representation of the previous words as input for the model. The previous words are looked up in a table that is learned simultaneously within the model. 
Figure~\ref{fig:Bengio} illustrates the model architecture.

\begin{figure}
    \centering
    \includegraphics[width =.8\textwidth]{img/bengio.png}
    \caption{The prediction-based Neural Language Model proposed by Bentio et al,. $C_{(i)}$ is the $i^{th}$ word embedding~\protect\cite{bengio2003neural}.}
    \label{fig:Bengio}
\end{figure}



These vectors are then fed to the hidden layer, and its output is passed on to the, so-called softmax layer, which predicts the next word of the sequence. A softmax layer is the final output layer of a neural network which performs multi-class classifications. The layer receives the output from the softmax function, which takes as an input a number of scores values from the previous layer and translates them into numbers in the range between 0 and 1, where the sum equals 1, resulting in a true representation of a probability distribution \cite{buduma2017fundamentals}. 
This method's problem lies within the size of the softmax layer, as the computing effort relies on the number of words in the vocabulary, which can be very high.

\textbf{Word2vec Embeddings} 
    \label{sec:Word2vec_embeddings}

A huge milestone were the, so called Word2vec embedding models introduced by Mikolov et al. in 2013 \cite{mikolov2013efficient}. 
Word2vec models generate distributed word representations by using either continuous bag-of-words or the continuous skip-gram model architectures. 
Bag-of-words models take $n$-words before and $n$-words after the target word into consideration for their prediction. The word order does not influence the prediction.  Mikolov et al. managed to efficiently improve the training by removing the hidden layer and approximating the objective. That way, they could improve training speed, accuracy and the efficiency of large-scale training of word embedding on huge amounts of text data.
Moreover, they found that the training on huge corpora enabled them to automatically capture relationships and similarities between words in text data, such as country - capital, or gender relationships, as shown in Figure~\ref{fig:Word2Vec}.

\begin{figure}
    \centering
    \includegraphics[ width = .8\textwidth]{img/Word2Vec_relations.png}
    \caption{Word2Vec representations capturing relationships and similarities between words, such as gender relationships, tenses and country-capital relations. Illustrated by~\protect\cite{Bujokas:2020}}
    \label{fig:Word2Vec}
\end{figure}


To summarize, the Word2vec is a shallow, two-layer neural network trained to generate a word embedding to represent linguistic contexts of words. It takes as an input a large text corpus and generates a vector space, consisting of several hundred dimensions, where each unique word in the corpus is assigned to a corresponding vector in the vector space. In the vector space, word vectors are positioned such that words that have common context in the corpus are located close to one another \cite{mikolov2013efficient}, as illustrated in Figure~\ref{fig:Word2Vec_bogdan}. The problem with Word2vec models is that they are not able to represent ambiguity of words.  
For example, the word "Apple" has the same representation in "Apple Pie" and "Apple Computer". 

\begin{figure}
    \centering
    \includegraphics[ width = .7\textwidth]{img/Wod2Vec_representations_embedding.png}
    \caption{A 2D t-SNE visualisation of Word2Vec embedding representations.~\protect\cite{Bogdan:2018}}
    \label{fig:Word2Vec_bogdan}
\end{figure}

\textbf{Recurrent Neural Networks}
    \label{sec:RNNs}
\todo{RNNs nochmal besser beschreiben}

Around 2014, Recurrent Neural Networks (RNN) were broadly adopted in the field of natural language processing \cite{jozefowicz2016exploring}.RNN are so-called Sequene-to-Sequence models that transform sequences of data (e.g. text, audio, video) into sequences of another type. Examples for the usage of these transformations are translation or classification tasks.

While in traditional NNs, all inputs and outputs are independent of each other, RNNs are recurrent, as it allows previous outputs to be used as inputs while having hidden states. In other words, it performs the same task for every sample of a sequence of input data, while the forwarded output being depended on the previous computations.
After calculating an output, it is copied and sent back into the recurrent network. For making a decision, it considers the given input plus the output stored form the previous input.
Therefore, RNNs use internal memory to process sequences of inputs, which makes them applicable to tasks such as handwriting recognition or natural language understanding.

(see Fig. RR1).

/ Fig RNN1 The same RNN block is applied recurrently to the input sequence. As the sequence's end is reached, the model has learned one vector that captures the meaning of the whole sentence. After that, the process is reverted in the decoder.

%TODO Figure

In practice, RNNs have shown to have difficulties processing long sentences, since they handle sequences word by word in a sequential fashion. This makes them prone to loose memory of distant positions in the sequence \cite{mikolov2010recurrent}.
Due to their vanishing and exploding gradient problem \cite{pascanu2013difficulty}, the RNN architecture was extended by long-short term memory networks (LSTM) \cite{hochreiter1997long, graves2005framewise}. 
LSTM networks are similar to RNNs, with the difference that they can process whole data sequences, instead of only single data points, which allows for improved determination of the storage of long-range dependencies in data. 

Latest NLP systems aim for the full understanding of text, checking syntactic and semantic validity of linguistic input, using real-world knowledge, speech acts, conversation and discourse structures \cite{kumar2011natural}.
Full understanding of text largely depends on the context it appears in. 
Deep contextual word representations have shown that they are able to model complex characteristics of word usage (e.g. syntax and semantics), and the variety of word usage in different linguistic contexts (e.g. polysemy) \cite{peters2018deep}. They generate a vector representation of each word that is based on the context (or the other words in the sentence) it appears in - to capture the word meaning in that context as well as other additional contextual information. Deep contextual word representations, first introduced by Peters et al. rely on a bidirectional LSTM, trained with a coupled language model objective on a large text corpus and are, therefore, called ELMo (Embeddings from Language Models) representations.
Instead of using fixed embedding representations, like Word2vec or GloVe \cite{pennington2014glove} for each word, ELMo "looks" at the entire sentence before assigning it an embedding. 

\textbf{Attention Mechanism}
    \label{sec:attention}
%Attention nochmal besser schreiben
\todo{Attention nochmal besser beschreiben}
In 2015 various papers described the concept of "attention" in training recurrent neural networks, allowing models to learn alignments between different modalities \cite{}. These models use information form learning to decide which parts of the input to pay attention to in order to draw global dependencies between input and output. 

The models are based on an encoder-decoder structure, where the encoder maps the input sequence of character representations to a sequence of continuous representations. In the second step, the decoder creates an output, a sequence of characters. 

The attention in the decoder allows it to look back at the hidden state of the source sequences, which are combined using a weighted average and then serve as an additional input to the decoder. 
An advanced approach of attention is called self-attention, which allows for obtaining more contextually sensitive word representations by looking at nearby words in a sentence or paragraph.

\textbf{Transformer Models}
    \label{sec:transformers}

In 2017 Vaswani et al. introduced the Transformer, a novel model architecture relying entirely on the attention mechanism \cite{vaswani2017attention}. They use stacked self-attention and point-wise, fully connected layers for en- and decoding. 
The self-attention based Transformer model is not recurrent, hence the output is not fed back as input like in RNNs but uses its attention to "look back" to the crucial piece of information. Text sequences are not memorized or stored completely but rather the parts, which are considered relevant. This approach allows the Transformer to outperform RNNs and LSTM networks and makes the model more parallelizable, resulting in enhanced computational performance. Transformer models proved to improve the state-of-the-art approaches in the field \cite{vaswani2017attention}. 

Famous Transformer models are ERNIE \cite{zhang2019ernie}, BERT \cite{jin2019bert}, XLnet \cite{yang2019xlnet}, RoBERTa \cite{liu2019roberta}, and DistilBERT \cite{sanh2019distilbert}. More detailed information about the Transformer Model BERT is provided in chapter~\ref{sec:bert}.

\subsubsection{Tasks in NLP}
    \label{sec:tasks_in_nLP}

Next to MT, there is a broad scope of tasks NLP is used for. Some prominent examples are speech recognition, language detection, part-of-speech tagging, sentiment analysis (SA), question answering, automatic summarization, text classification, character recognition, textual entailment, and many more.

NLP tasks can be divided in two main fields: analysis, and generation tasks. Analysis tasks aim for analysing existing text, while the objective of generation tasks is to generate new text. The analysis task can again be divided into syntactic, a language structure-based task, semantic, a meaning based task and pragmatic, a task of open difficult problems to solve \cite{ganegedara2018natural}. 
Figure ~\ref{fig:NLP-Tasks-Taxonomy} illustrates the categorization of different NLP task as described provided by Ganegadara. 
\begin{figure}
    \centering
    \includegraphics[ width = .7\textwidth]{img/NLP_tasks.jpg}
    \caption{A taxonomy of popular NLP tasks~\protect\cite{ganegedara2018natural}}
    \label{fig:NLP-Tasks-Taxonomy}
\end{figure}
Examples for semantic tasks are SA and Aspect-based Sentiment Analysis (ABSA). 
SA is an active field of research in NLP that aims to detect opinions in (mostly user-generated) text. The perceived sentiment, which occurs in user comments, feedback questionnaires, or critiques, provides valuable information for various purposes. Sentiment is usually expressed in either two categories: positive and negative, or on an $n$-point scale (e.g., very good, good, satisfactory, sufficient, insufficient). Thus, a sentiment analysis task can be interpreted as a classification task, where each class represents a sentiment \cite{prabowo2009sentiment}. Figure ~\ref{fig:SA} shows an example of the sentiment analysis task using a BERT language model.

\begin{figure}
    \centering

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,218); %set diagram left start at 0, and has height of 218

%Shape: Rectangle [id:dp7006294352286838] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (14.5,104.13) -- (194.3,104.13) -- (194.3,177.13) -- (14.5,177.13) -- cycle ;
%Straight Lines [id:da6575723888982618] 
\draw    (212.23,143.13) -- (251.23,143.13) ;
\draw [shift={(253.23,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp8363863646203258] 
\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (261,113) .. controls (261,105.27) and (267.27,99) .. (275,99) -- (317,99) .. controls (324.73,99) and (331,105.27) .. (331,113) -- (331,168.13) .. controls (331,175.87) and (324.73,182.13) .. (317,182.13) -- (275,182.13) .. controls (267.27,182.13) and (261,175.87) .. (261,168.13) -- cycle ;
%Straight Lines [id:da9521339561624853] 
\draw    (339.23,143.13) -- (378.23,143.13) ;
\draw [shift={(380.23,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp0334875358297424] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (393,113) .. controls (393,105.27) and (399.27,99) .. (407,99) -- (449,99) .. controls (456.73,99) and (463,105.27) .. (463,113) -- (463,168.13) .. controls (463,175.87) and (456.73,182.13) .. (449,182.13) -- (407,182.13) .. controls (399.27,182.13) and (393,175.87) .. (393,168.13) -- cycle ;
%Straight Lines [id:da2921180358184613] 
\draw    (474.73,143.13) -- (513.73,143.13) ;
\draw [shift={(515.73,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp439847396677838] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (528,114.93) -- (550.4,114.93) -- (550.4,133) -- (528,133) -- cycle ;
%Shape: Rectangle [id:dp6078072626477552] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (528,133) -- (550.4,133) -- (550.4,151.07) -- (528,151.07) -- cycle ;
%Shape: Rectangle [id:dp7563474548449828] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (528,151.07) -- (550.4,151.07) -- (550.4,169.13) -- (528,169.13) -- cycle ;

% Text Node
\draw (63,51) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input\\\textcolor[rgb]{0.25,0.46,0.02}{Features}};
% Text Node
\draw (21,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize This is my favourite laptop. }\\{\scriptsize I would recomment it to anyone!}};
% Text Node
\draw (269,116) node [anchor=north west][inner sep=0.75pt]   [align=left] {BERT \\{\tiny language model}};
% Text Node
\draw (396,113) node [anchor=north west][inner sep=0.75pt]   [align=left] {Classifier};
% Text Node
\draw (555,114) node [anchor=north west][inner sep=0.75pt]   [align=left] {POS};
% Text Node
\draw (555,133) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEU};
% Text Node
\draw (556,154) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEG};
% Text Node
\draw (519,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {Output\\\textcolor[rgb]{0.25,0.46,0.02}{Prediction}};
% Text Node
\draw (397,132) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny Feed-forward }};
% Text Node
\draw (396,141) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny neural network }};
% Text Node
\draw (397,150) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny + softmax}};

\end{tikzpicture}
    \caption{Sentiment Analysis of Laptop Reviews}
    \label{fig:SA}
\end{figure}


In contrast to SA, ABSA is a more fine-grained task, which consists of two main subtasks, namely Aspect-Category Detection, and Aspect-Category Sentiment Classification. In the first (multi-label classification \footnote{in single-label classification one sample in the data contains exactly one output label, while in the multi-label classification task, one sample can contain n output labels.}) task, a sentence is associated with a set of predefined single- and multi-word aspect terms (e.g., "operation performance", "battery"). The second task, the Aspect-Category Sentiment Classification, deals with the detection of sentiment associated with its aspect (e.g. "positive", "neutral", "negative") \cite{pavlopoulos2014aspect}. See Figure ~\ref{fig:ABSA} for an illustration of the ABSA task.


\begin{figure}
    \centering

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,341); %set diagram left start at 0, and has height of 341

%Shape: Rectangle [id:dp7006294352286838] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (14.5,104.13) -- (194.3,104.13) -- (194.3,177.13) -- (14.5,177.13) -- cycle ;
%Straight Lines [id:da6575723888982618] 
\draw    (212.23,143.13) -- (251.23,143.13) ;
\draw [shift={(253.23,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp8363863646203258] 
\draw  [fill={rgb, 255:red, 248; green, 231; blue, 28 }  ,fill opacity=1 ] (261,113) .. controls (261,105.27) and (267.27,99) .. (275,99) -- (317,99) .. controls (324.73,99) and (331,105.27) .. (331,113) -- (331,168.13) .. controls (331,175.87) and (324.73,182.13) .. (317,182.13) -- (275,182.13) .. controls (267.27,182.13) and (261,175.87) .. (261,168.13) -- cycle ;
%Straight Lines [id:da9521339561624853] 
\draw    (339.23,143.13) -- (378.23,143.13) ;
\draw [shift={(380.23,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Rounded Rect [id:dp0334875358297424] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (393,113) .. controls (393,105.27) and (399.27,99) .. (407,99) -- (449,99) .. controls (456.73,99) and (463,105.27) .. (463,113) -- (463,168.13) .. controls (463,175.87) and (456.73,182.13) .. (449,182.13) -- (407,182.13) .. controls (399.27,182.13) and (393,175.87) .. (393,168.13) -- cycle ;
%Straight Lines [id:da2921180358184613] 
\draw    (474.73,143.13) -- (513.73,143.13) ;
\draw [shift={(515.73,143.13)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;
%Shape: Rectangle [id:dp439847396677838] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (584,91.93) -- (606.4,91.93) -- (606.4,110) -- (584,110) -- cycle ;
%Shape: Rectangle [id:dp6078072626477552] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (584,110) -- (606.4,110) -- (606.4,128.07) -- (584,128.07) -- cycle ;
%Shape: Rectangle [id:dp7563474548449828] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (584,128.07) -- (606.4,128.07) -- (606.4,146.13) -- (584,146.13) -- cycle ;
%Shape: Rectangle [id:dp25079314597811353] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (584,155.93) -- (606.4,155.93) -- (606.4,174) -- (584,174) -- cycle ;
%Shape: Rectangle [id:dp724072679501146] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (584,174) -- (606.4,174) -- (606.4,192.07) -- (584,192.07) -- cycle ;
%Shape: Rectangle [id:dp120836736470876] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (584,192.07) -- (606.4,192.07) -- (606.4,210.13) -- (584,210.13) -- cycle ;
%Shape: Rectangle [id:dp6243646913162751] 
\draw   (514.3,109) -- (578,109) -- (578,131.47) -- (514.3,131.47) -- cycle ;
%Shape: Rectangle [id:dp06781285859521458] 
\draw   (514.3,170) -- (578,170) -- (578,192.47) -- (514.3,192.47) -- cycle ;

% Text Node
\draw (63,51) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input\\\textcolor[rgb]{0.25,0.46,0.02}{Features}};
% Text Node
\draw (21,124) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize This is my favourite laptop. }\\{\scriptsize I would recomment it to anyone!}};
% Text Node
\draw (269,116) node [anchor=north west][inner sep=0.75pt]   [align=left] {BERT \\{\tiny language model}};
% Text Node
\draw (396,113) node [anchor=north west][inner sep=0.75pt]   [align=left] {Classifier};
% Text Node
\draw (611,91) node [anchor=north west][inner sep=0.75pt]   [align=left] {POS};
% Text Node
\draw (611,110) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEU};
% Text Node
\draw (612,131) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEG};
% Text Node
\draw (509,50) node [anchor=north west][inner sep=0.75pt]   [align=left] {Output\\\textcolor[rgb]{0.25,0.46,0.02}{Prediction}};
% Text Node
\draw (397,132) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny Feed-forward }};
% Text Node
\draw (396,141) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny neural network }};
% Text Node
\draw (397,150) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny + softmax}};
% Text Node
\draw (611,155) node [anchor=north west][inner sep=0.75pt]   [align=left] {POS};
% Text Node
\draw (611,174) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEU};
% Text Node
\draw (612,195) node [anchor=north west][inner sep=0.75pt]   [align=left] {NEG};
% Text Node
\draw (589,218) node [anchor=north west][inner sep=0.75pt]   [align=left] {...};
% Text Node
\draw (547,205) node [anchor=north west][inner sep=0.75pt]   [align=left] {...};
% Text Node
\draw (521.3,109) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny Aspect Class 1}};
% Text Node
\draw (521.3,171) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\tiny Aspect Class 2}};


\end{tikzpicture}
    \caption{Aspect Based Sentiment Analysis of Laptop Reviews}
    \label{fig:ABSA}
\end{figure}



%-------------------------------------------------------------------
%----------------------------BERT-----------------------------------
%------------------------------------------------------------------


\subsection{Bidirectional Encoder Representations from Transformers - BERT}
    \label{sec:bert}
In 2018, Google AI researchers achieved a breakthrough in NLP by introducing BERT, a deep contextual language representation model. BERT is an abbreviation for Bidirectional Encoder Representations from Transformers \cite{devlin2018bert}. It is built on preceding innovations: deep contextualized word representations \cite{peters2018deep}, the Transformer architecture \cite{vaswani2017attention}, and pre-training on a language modeling task with subsequent fine-tuning on a downstream task \cite{radford2018improving, howard2018universal}.




\begin{figure}
    \centering
    \includegraphics[ width = .8\textwidth]{img/bert-pre-training_fine-tuning.png}
    \caption{The BERT workflow consists of (1) pre-training: self-supervised training on a large amount of text (books and Wikipedia) and (2) fine-tuning: supervised training on a downstream task with labeled data. Illustration from~\protect\cite{alammar2018illustrated}.}
    \label{fig:BERT}
\end{figure}


\subsubsection{Model Architecture}
    \label{sec:bert_architecture}
% elaborate
BERT is a stack of transformer encoder layers that consist of a multi-head attention layer (heads), followed by a fully-connected neural networks.
For every input token in a text sequence,( e.g., a sentence), each head computes key, value, and query vectors, which are then used to create a weighted representation. The outputs of all heads in the same layer are combined and run through a fully-connected layer.
\

BERT uses the self-attention mechanism introduced with the Transformer model \cite{vaswani2017attention} and described in chapter~\ref{sec:attention}. 
%Fig xy exposes an example of how BERT uses the self-attention mechanism....


Similar to the Word2vec word embedding, BERT does map related words closely, but it is context-sensitive, meaning a different word vector is computed for a word if encountered in different contexts. In other terms, related words have a smaller distance in the word embedding than unrelated words, but can be  discriminated by their context as BERT makes use of the information of the tokens in the sequence surrounding the word of interest.

To compute the input-encoding, BERT first tokenizes the given sentences into referring word pieces and then combines three input embedding layers (token, position, and segment) to obtain a fixed-length vector per token. Figure ~\ref{fig:BERT_Layers} gives an overview of the embedding layer architecture.

\textbf{Vocabulary Generation}

An important part of producing word embeddings is tokenization. As stated before, a word embedding is a feature vector representation of a word. A feature vector is a list of values generated to represent one word, where the values are floating points and can be positive and negative. The distance of word embeddings displays the distance of word similarity. To give an example, the word embedding for the word "cat" could be

\texttt{<0.4125, -1.6098, 0.6047, ..., -1.4257, -1.231>}.


BERT has a fixed set of learned word embeddings, composed in a word embedding lookup-table. Every row in this table corresponds to one word, and every column represents a feature of one token.
BERT large has a vocabulary of 30,522 tokens and 786 features in its embedding. For simplicity, Figure ~\ref{fig:word_embedding} shows a table of only 10 words tokens and 5 features.

\begin{figure}
    \centering
    
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,412); %set diagram left start at 0, and has height of 412

%Shape: Rectangle [id:dp7006294352286838] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (42.5,97.65) -- (234.5,97.65) -- (234.5,245.65) -- (42.5,245.65) -- cycle ;
%Notched Right Arrow [id:dp3093746656409001] 
\draw   (143.13,148.46) -- (143.68,169.75) -- (148.51,169.63) -- (139.21,184.07) -- (129.17,170.12) -- (134.01,170) -- (133.46,148.7) -- (138.42,153.42) -- cycle ;
%Shape: Grid [id:dp4237249967627763] 
\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (448,46) -- (548.45,46) -- (548.45,246.48) -- (448,246.48) -- cycle ; \draw   (448,46) -- (448,246.48)(468,46) -- (468,246.48)(488,46) -- (488,246.48)(508,46) -- (508,246.48)(528,46) -- (528,246.48)(548,46) -- (548,246.48) ; \draw   (448,46) -- (548.45,46)(448,66) -- (548.45,66)(448,86) -- (548.45,86)(448,106) -- (548.45,106)(448,126) -- (548.45,126)(448,146) -- (548.45,146)(448,166) -- (548.45,166)(448,186) -- (548.45,186)(448,206) -- (548.45,206)(448,226) -- (548.45,226)(448,246) -- (548.45,246) ; \draw    ;
%Bend Arrow [id:dp1033529280692509] 
\draw   (190.67,206.82) -- (190.67,206.82) .. controls (223.32,250.05) and (284.84,258.63) .. (328.08,225.98) -- (405.77,167.31) -- (408.45,170.85) -- (415.43,158.15) -- (401.3,161.38) -- (403.98,164.93) -- (326.28,223.6) .. controls (284.36,255.26) and (224.71,246.94) .. (193.04,205.02) -- (193.04,205.02) -- cycle ;
%Shape: Grid [id:dp17330989415915177] 
\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (79,332) -- (179.67,332) -- (179.67,352.82) -- (79,352.82) -- cycle ; \draw   (79,332) -- (79,352.82)(99,332) -- (99,352.82)(119,332) -- (119,352.82)(139,332) -- (139,352.82)(159,332) -- (159,352.82)(179,332) -- (179,352.82) ; \draw   (79,332) -- (179.67,332)(79,352) -- (179.67,352) ; \draw    ;
%Shape: Grid [id:dp2416280342144621] 
\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (194,332) -- (294.67,332) -- (294.67,352.82) -- (194,352.82) -- cycle ; \draw   (194,332) -- (194,352.82)(214,332) -- (214,352.82)(234,332) -- (234,352.82)(254,332) -- (254,352.82)(274,332) -- (274,352.82)(294,332) -- (294,352.82) ; \draw   (194,332) -- (294.67,332)(194,352) -- (294.67,352) ; \draw    ;
%Shape: Grid [id:dp9264270742131507] 
\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (329,332) -- (429.67,332) -- (429.67,352.82) -- (329,352.82) -- cycle ; \draw   (329,332) -- (329,352.82)(349,332) -- (349,352.82)(369,332) -- (369,352.82)(389,332) -- (389,352.82)(409,332) -- (409,352.82)(429,332) -- (429,352.82) ; \draw   (329,332) -- (429.67,332)(329,352) -- (429.67,352) ; \draw    ;
%Shape: Grid [id:dp20800430109446355] 
\draw  [draw opacity=0][fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (471,333) -- (571.67,333) -- (571.67,353.82) -- (471,353.82) -- cycle ; \draw   (471,333) -- (471,353.82)(491,333) -- (491,353.82)(511,333) -- (511,353.82)(531,333) -- (531,353.82)(551,333) -- (551,353.82)(571,333) -- (571,353.82) ; \draw   (471,333) -- (571.67,333)(471,353) -- (571.67,353) ; \draw    ;

% Text Node
\draw (104,70) node [anchor=north west][inner sep=0.75pt]   [align=left] {Dictionary};
% Text Node
\draw (104,121) node [anchor=north west][inner sep=0.75pt]   [align=left] {word string};
% Text Node
\draw (112,195) node [anchor=north west][inner sep=0.75pt]   [align=left] {word ID};
% Text Node
\draw (453,23) node [anchor=north west][inner sep=0.75pt]   [align=left] {Lookup Table};
% Text Node
\draw (432,45) node [anchor=north west][inner sep=0.75pt]   [align=left] {0};
% Text Node
\draw (127,303) node [anchor=north west][inner sep=0.75pt]   [align=left] {I};
% Text Node
\draw (450,249) node [anchor=north west][inner sep=0.75pt]   [align=left] {0 \ \ 1 \ \ 2 \ \ 3 \ \ 4 };
% Text Node
\draw (235,305) node [anchor=north west][inner sep=0.75pt]   [align=left] {like};
% Text Node
\draw (365,307) node [anchor=north west][inner sep=0.75pt]   [align=left] {straw};
% Text Node
\draw (487,308) node [anchor=north west][inner sep=0.75pt]   [align=left] {\#\#berries};
% Text Node
\draw (432,65) node [anchor=north west][inner sep=0.75pt]   [align=left] {1};
% Text Node
\draw (432,86) node [anchor=north west][inner sep=0.75pt]   [align=left] {2};
% Text Node
\draw (432,106) node [anchor=north west][inner sep=0.75pt]   [align=left] {3};
% Text Node
\draw (432,127) node [anchor=north west][inner sep=0.75pt]   [align=left] {4};
% Text Node
\draw (431,149) node [anchor=north west][inner sep=0.75pt]   [align=left] {5};
% Text Node
\draw (432,171) node [anchor=north west][inner sep=0.75pt]   [align=left] {6};
% Text Node
\draw (432,191) node [anchor=north west][inner sep=0.75pt]   [align=left] {7};
% Text Node
\draw (432,212) node [anchor=north west][inner sep=0.75pt]   [align=left] {8};
% Text Node
\draw (433,232) node [anchor=north west][inner sep=0.75pt]   [align=left] {9};


\end{tikzpicture}
\caption{The conversion of a sequence string into its word vector representations.}
    \label{fig:word_embedding}

\end{figure}

A dictionary converts a word string into the corresponding word-id in the word embedding lookup table. 
When feeding a sequence of words into the BERT architecture, it will fist look up the word-ids in the dictionary and then take the corresponding word embeddings to get a sequence of word embeddings.
The Second Edition of the 20th volume Oxford English Dictionary contains entries for 171,476 words \cite{dictionary1989oxford}. In order to keep the size of the vocabulary as small as possible, not all words are included in the vocabulary. A WordPiece model breaks down unknown words into subwords \cite{wu2016google}. Given the word $embedding$. There is no representation for this word in the dictionary. The WordPiece method breaks the word down into the three word pieces $em$, $\#\#bed$, and $\#\#ding$, which are entries in the dictionary. That way the the sequence, the word "embedding" appears in, consists of two additional tokens. Except for the first token, subwords are marked with two hash-symbols in front of the characters. Therefore, the subword $bed$ in the word $bedding$ does not have the same vector representation as the one in the word $embedding$ since it is the first token of the word and does not contain the two hash symbols. The WordPiece representation for the word $bedding$ is $bed$, $\#\#ding$, which conveys the information, that the word $bedding$ is related to the word $bed$.
The words stored in the dictionary differs, depending on the corpus, used for vocabulary generation.

\begin{figure}
    \centering
    \includegraphics[ width = 1\textwidth]{img/bert_input_representatino.png}
    \caption{Three BERT embedding layers architecture: token, sequence, and positional embedding layers \cite{devlin2018bert}.}
    \label{fig:BERT_Layers}
\end{figure}
%----------------Token Embedding Layer------------------------------



\textbf{Token Embedding Layer}

The purpose of the token embedding layer is to transform the tokenized input (e.g. "I like strawberries") into a vector representation of fixed dimensions, as shown in Figure ~\ref{fig:BERT_tokenEmbedding}. In case of BERT base, each word is represented as a 768-dimensional vector.

Nora: Also werden quasi diese Layers Parameter random(oder nach einem Schema) initialisiert) und mitgelernt... 

\begin{figure}
\centering
    \begin{tikzpicture}[replace stretch/.style args={from #1 to #2 by #3}{%
        /utils/exec=\pgfmathsetmacro{\offlen}{#2-#1},
        dash pattern=on #1 off \offlen pt on 10cm,
        postaction={#3,dash pattern=on 0pt off #1 on \offlen pt off 10cm}}]
        
        % nodes
        \node[align=center] (start) [] {{\Large "I like strawberries"}\\\textit{3 words}};
        \node[align=center] (tokenized) [below of=start, yshift=-2cm] {{\Large"[CLS]", "I", "like", "straw", "\#\#berries", "[SEP]"}\\\textit{6 tokens}};
        \node (vectorized) [below of=tokenized, yshift=-2cm, draw, thick, minimum height=1.5cm, minimum width=3.5cm] {Token Embeddings};
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}, asymbrace=0.75]
        (vectorized.south west) -- (vectorized.south east) node (vectorized_south)[black,midway,xshift=0.9cm, yshift=-0.5cm] {$768$};
        \draw [decorate,decoration={brace,amplitude=7pt}]
        (vectorized.north east) -- (vectorized.south east) node (vectorized_east)[black,midway,xshift=0.9cm, xshift=0.1cm, align=center]
        {$30.522$\\"words"};
        
        % tokenized object
        \node (0) [below of=vectorized, xshift=-5.5cm, yshift=-4.5cm, draw, thick, minimum height=1cm, minimum width=1.5cm] {\small [CLS]};
        \node (1) [right of=0, draw, xshift=0.5cm, thick, minimum height=1cm, minimum width=1.5cm] {\small I};
        \node (2) [right of=1, draw, xshift=0.5cm, thick, minimum height=1cm, minimum width=1.5cm] {\small love};
        \node (3) [right of=2, draw, xshift=0.5cm, thick, minimum height=1cm, minimum width=1.5cm] {\small straw};
        \node (4) [right of=3, draw, xshift=0.5cm, thick, minimum height=1cm, minimum width=1.5cm] {\small \#\#berries};
        \node (5) [right of=4, draw, xshift=0.5cm, thick, minimum height=1cm, minimum width=1.5cm] {\small [SEP]};
    
        \draw[thick,leaveout] (0.north west) -- ++(3,3);
        \draw[thick,leaveout] (1.north west) -- ++(3,3);
        \draw[thick,leaveout] (2.north west) -- ++(3,3);
        \draw[thick,leaveout] (3.north west) -- ++(3,3);
        \draw[thick,leaveout] (4.north west) -- ++(3,3);
        \draw[thick,leaveout] (5.north west) -- ++(3,3);
        \draw[thick,leaveout] (5.north east) -- ++(3,3);
        \draw[thick, replace stretch={from 2.16cm to 2.82cm by {-,draw=none}}] (5.south east) -- ++(3.03,3.03);
        
        \draw[thick] ([xshift=0.5cm, yshift=0.5cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=1cm, yshift=1cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=1.5cm, yshift=1.5cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=2cm, yshift=2cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=2.5cm, yshift=2.5cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=3cm, yshift=3cm] 0.north west) -- ++(9.05,0) -- ++(0,-1);
        
        \draw[thick] ([xshift=2cm, yshift=2cm] 0.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 1.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 2.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 3.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 4.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 5.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 5.south east) -- ++(-0.49,0);
        
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}]
        (0.south west) -- (5.south east) node (vectorized_south)[black,midway,xshift=0cm, yshift=-0.5cm] {$6$};
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}]
        (5.south east) -- ++(3,3) node (vectorized_south)[black,midway,xshift=0.4cm, yshift=-0.4cm] {\begin{turn}{45}$768$\end{turn}};
        \node at ([xshift=1.75cm, yshift=1.75cm] 5.south east) {\begin{turn}{45}\dots\end{turn}};
                
        % arrows
        \draw[->,thick] (start) -- node[right] () {\textbf{tokenization}} (tokenized.north);
        \draw[->,thick] (tokenized) -- node[right] () {\textbf{vector representation}} (vectorized.north);
        \draw[->,thick] (vectorized) -- ++(0,-2cm);

    \end{tikzpicture}
\caption{BERTbase Token Embedding Layer.} 
\label{fig:BERT_tokenEmbedding}
\end{figure}

Special tokens are needed for sentence classification. $"[CLS]"$ is added in front of every input example, and $"[SEP]"$ serves as a separator token.
As mentioned before, the tokenization method WordPiece, is used, which in this case splits the word "strawberries" into "straw" and "\#\#berries". 

%--------------Segment Embedding Layer------------------------------
\textbf{Segment Embedding Layer}

The purpose of the segment embedding layer is to enable BERT to distinguish inputs of a given sentence pair. Sentence pairs are used for example for the task of question answering, where text of interest and the question are differentiated like in "[CLS] He likes strawberries [SEP] What does he like?"
The segment embedding layer uses two indices 0 and 1 as vector representations to assign tokens to their corresponding input segment, see Figure \ref{fig:BERT_segmentEmbedding}.

\begin{figure}
    \centering
\begin{tikzpicture}[replace stretch/.style args={from #1 to #2 by #3}{%
        /utils/exec=\pgfmathsetmacro{\offlen}{#2-#1},
        dash pattern=on #1 off \offlen pt on 10cm,
        postaction={#3,dash pattern=on 0pt off #1 on \offlen pt off 10cm}}]
        
        % nodes
        \node[align=center] (start) [] {{\Large "I like strawberries"} \\ {\Large "I like ice cream"}\\\textit{2 inputs}};
        \node[align=center] (tokenized) [below of=start, yshift=-2cm] {{\Large"[CLS]", "I", "like", "straw", "\#\#berries", "[SEP]", "I", "like" "ice" "cream"}\\\textit{10 tokens}};
        \node (vectorized) [below of=tokenized, yshift=-2cm, draw, thick, minimum height=1.5cm, minimum width=3.5cm] {Segment Embeddings};
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}, asymbrace=0.75]
        (vectorized.south west) -- (vectorized.south east) node (vectorized_south)[black,midway,xshift=0.9cm, yshift=-0.5cm] {$768$};
        \draw [decorate,decoration={brace,amplitude=7pt}]
        (vectorized.north east) -- (vectorized.south east) node (vectorized_east)[black,midway,xshift=0.4cm, xshift=0.1cm, align=center]
        {$2$};
        
        % tokenized object
        \node (0) [below of=vectorized, xshift=-6.5cm, yshift=-4.5cm, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (1) [right of=0, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (2) [right of=1, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (3) [right of=2, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (4) [right of=3, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (5) [right of=4, draw, thick, minimum height=1cm, minimum width=1cm] {\small 0};
        \node (6) [right of=5, draw, thick, minimum height=1cm, minimum width=1cm] {\small 1};
        \node (7) [right of=6, draw, thick, minimum height=1cm, minimum width=1cm] {\small 1};
        \node (8) [right of=7, draw, thick, minimum height=1cm, minimum width=1cm] {\small 1};
        \node (9) [right of=8, draw, thick, minimum height=1cm, minimum width=1cm] {\small 1};
    
        \draw[thick,leaveout] (0.north west) -- ++(3,3);
        \draw[thick,leaveout] (1.north west) -- ++(3,3);
        \draw[thick,leaveout] (2.north west) -- ++(3,3);
        \draw[thick,leaveout] (3.north west) -- ++(3,3);
        \draw[thick,leaveout] (4.north west) -- ++(3,3);
        \draw[thick,leaveout] (5.north west) -- ++(3,3);
        \draw[thick,leaveout] (5.north west) -- ++(3,3);
        \draw[thick,leaveout] (6.north west) -- ++(3,3);
        \draw[thick,leaveout] (7.north west) -- ++(3,3);
        \draw[thick,leaveout] (8.north west) -- ++(3,3);
        \draw[thick,leaveout] (9.north west) -- ++(3,3);
        \draw[thick,leaveout] (9.north east) -- ++(3,3);
        \draw[thick, replace stretch={from 2.16cm to 2.82cm by {-,draw=none}}] (9.south east) -- ++(3.03,3.03);
        
        \draw[thick] ([xshift=0.5cm, yshift=0.5cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=1cm, yshift=1cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=1.5cm, yshift=1.5cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=2cm, yshift=2cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=2.5cm, yshift=2.5cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        \draw[thick] ([xshift=3cm, yshift=3cm] 0.north west) -- ++(10.05,0) -- ++(0,-1);
        
        \draw[thick] ([xshift=2cm, yshift=2cm] 0.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 1.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 2.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 3.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 4.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 5.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 6.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 7.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 8.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 9.north west) -- ++(0,-0.5);
        \draw[thick] ([xshift=2cm, yshift=2cm] 9.south east) -- ++(-0.49,0);
        
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}]
        (0.south west) -- (9.south east) node (vectorized_south)[black,midway,xshift=0cm, yshift=-0.5cm] {$10$};
        \draw [decorate,decoration={brace,amplitude=7pt, mirror}]
        (9.south east) -- ++(3,3) node (vectorized_south)[black,midway,xshift=0.4cm, yshift=-0.4cm] {\begin{turn}{45}$768$\end{turn}};
        \node at ([xshift=1.75cm, yshift=1.75cm] 9.south east) {\begin{turn}{45}\dots\end{turn}};
                
        % arrows
        \draw[->,thick] (start) -- node[right] () {\textbf{concat \& tokenize}} (tokenized.north);
        \draw[->,thick] (tokenized) -- node[right] () {\textbf{lookup vector representation}} (vectorized.north);
        \draw[->,thick] (vectorized) -- ++(0,-2cm);

    \end{tikzpicture}
    \caption{BERTbase Segment Embedding Layer.}
    \label{fig:BERT_segmentEmbedding}
\end{figure}

%----------------Position Embedding Layer----------------------------

\textbf{Position Embedding Layer}

The position embedding layer leverages the encoding of positional information in an input sequence inside a Transformer. BERT processes input sequences up to a length of 512. Therefore the position embedding layer is a lookup table of size 512x768, where the each row is a vector representation of the word in the corresponding position. In an input like "I like you" and "You like me", both "I" and "You" will have identical position embeddings since they are the first token in the input sequence. 

The two versions of BERT (base and large), differ in the number of layers, their hidden size, and the number of attention heads.  

\subsubsection{Pre-Training and Fine-Tuning BERT}
    \label{sec:bert_pre-fine}
The BERT workflow conventionally consists of (1) pre-training and (2) finetuning. Pre-training uses two self-supervised tasks: masked language modeling (MLM) and next sentence prediction (NSP) for learning deep bidirectional word representations from unlabeled text data.

As explained, the original Transformer model consists of an encoder, reading in the text input, and a decoder, generating predictions for a generative sequence to sequence task. The BERT model only contains the encoder since the goal is to generate a language model, solely for classification tasks.

The input for the Transformer encoder is a sequence of symbol representations or tokens, first embedded into vectors, and then processed in the network. For each input sequence vector, BERT generates an output sequence vector with the same index.

The B in BERT represents the word bidirectional and refers to the reading of the text input. While directional models read text sequentially, meaning left to right, or right to left \cite{peters2018deep, radford2018improving}, bidirectional models take an entire sequence of words into consideration for their prediction, what allows them to learn the context of a word, based on those around it in the sequence.

\textbf{Pre-Training - MLM}

Pre-Training is performed using a Masked Language Modeling task (MLM).
When solving the MLM task, 15\% of the words in a sequence are replaced with a [MASK] token. Based on the context of the remaining words, the model tries to predict the original value of the masked words. To do so, a classification layer is added on top of the encoder output, multiplying the output vector with the embedding matrix. The product of the calculation is represented in a vocabulary dimension. After that, the probability for each word in the vocabulary is calculated using the softmax.

\begin{figure}
    \centering
    \includegraphics[ width = 1\textwidth]{img/BERT-MLM_3.png}
    \caption{The first task of the self-supervised pre-training process of BERT: Masked Language Modelling. Illustration from~\protect\cite{alammar2018illustrated}}
    \label{fig:BERT_MLM}
\end{figure}


\textbf{Pre-Training - NSP}

In addition to MLM, NSP is used to further improve model pre-training.
For the task of NSP, the model takes two pairs of sentences as input and attempts to predict if the second sentence is subsequent to the first one. In 50\% of the cases, the second sentence of the input pair is the correct subsequent sentence, while in the other 50\%, it is a random sentence from the text corpus.
To solve this task, a [CLS] token is inserted as the first token of the first sentence and a [SEP] token at the end of both sentences. A sentence embedding\footnote{a dictionary of sentence: index letter (A and B)} with a size of 2 (for the two sentences) is used to give information about the sentence order. Moreover, to indicate the position of the token in the sequence, a positional embedding is used. Fig~\ref{fig:BERT_NSP} displays the processes best.
\begin{figure}
    \centering
    \includegraphics[ width = .9\textwidth]{img/BERT_NSP.png}
    \caption{The second task of the self-supervised pre-training process of BERT: Next Sentence Prediction. Illustration from~\protect\cite{alammar2018illustrated}.}
    \label{fig:BERT_NSP}
\end{figure}

When solving the NSP task, the model first feeds the entire input sequence into the Transformer model, then shapes the output of the [CLS] token into a 2x1 shaped vector and calculates the probability of two sentences being subsequent using the softmax layer.
MLM and NSP are trained simultaniasly while pre-training, which allows for minimizing the combined loss function.

The BERTbase model is pre-trained on a Wikipedia dump and book corpus \cite{zhu2015aligning}, a dataset containing over 10,000 books of different genres. 

Improved versions of BERT vary pre-training tasks and have shown to achieve better performances in certain areas. E.g. the RoBERTa model is pre-trained on the MLM task only \cite{liu2019roberta}. 
 
\textbf{Language Model Fine-tuning}

In fine-tuning for downstream tasks, one or more fully-connected layers are added on top of the pre-trained model's final encoder layer, resulting in a good performance for a broad range of NLP tasks \cite{devlin2018bert}. 

Fine-tuning BERT for down-stream tasks is computationally inexpensive, compared to pre-training. 
The pre-training objectives of MLM and NSP allow it be used on various sequence-pair and single sequence tasks without applying task-specific model architecture alterations. BERT can be fine-tuned on a specific downstream task by swapping out the respective task-specific inputs and outputs. The final step is fine-tuning the parameters end-to-end for an appropriate number of epochs using labeled data from the downstream task. Figure ~\ref{fig:BERT_downstream} shows the fine-tuning of BERT on different downstream tasks. 
At the input, sentence A and B form the pre-training task NSP are comparable with:
\begin{itemize}
\item sentence pairs in paraphrasing
\item hypothesis-premise pairs in entailment
\item question-passage pairs in question answering
\item a degenerate text -$\emptyset$ pair in text classification or sequence tagging.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[ width = .9\textwidth]{img/BERT_downstream-tasks.png}
    \caption{Fine-tuning the BERT model on multiple downstream tasks~\protect\cite{devlin2018bert}.}
    \label{fig:BERT_downstream}
\end{figure}


For sequence tagging or question answering, the token representations are fed into an output layer for token-level tasks, while for classification tasks such as entailment or sentiment analysis, the [CLS] token is fed into an output layer for classification \cite{devlin2018bert}. 

\todo{redo adv examples (zum Schluss)}

\begin{figure}
    \centering
    \includegraphics[ width = .9\textwidth]{img/BERT-Finetuning.png}
    \caption{Finetuning of BERT for a simple classification task detecting spam in E-Mails. Illustration from~\protect\cite{alammar2018illustrated}}
    \label{fig:BERT_NSP}
\end{figure}

\subsection{Adversarial Examples}
    \label{sec:adversarial_examples}

In 2013, \cite{szegedy2013intriguing}  were the first to discover the vulnerability of several machine learning models through adversarial examples: inputs crafted by adversaries with the intent of causing deep
neural networks to misclassify \cite{papernot2016crafting}. These carefully curated examples are correctly classified by a human observer but can fool a target model, raising serious concerns regarding the security of existing machine learning algorithms. 
The altered input data is crafted from a valid sample and either the model's gradients or its output data. 
To describe adversarial examples in a formalized manner, a classifier can be defined as a function \(C(x) = y\). This function has the input value \(x\) and the output value \(y\). After an attacker modifies the input value \(x\), it will become \(x'\), which is incorrectly classified by the algorithm and leads to \(C(x) \neq C(x')\). 
The difference between \(x\) and \(x'\) is commonly called the distance matrix \cite{carlini2017towards}. 
See Figure \ref{fig:advEx} for an example of an applied adversarial perturbation. The image of a panda, which is originally classified correctly with the label "panda" will be misclassified as "gibbon" after adding imperceptible noises. 
\begin{figure}
    \centering
    \includegraphics[ width = .8\textwidth]{img/advExample.jpg}
    \caption{Adversarial example in computer vision.~\protect\cite{papernot2016crafting}. Original image of a panda bear shown on the lift side. The right side shows the misclassified image with the added perturbation, shown in the middle.}
    \label{fig:advEx}
\end{figure}

The notation "adversarial example" is used in follow-up research to denote all kinds of perturbation samples in a general manner. In further research, \cite{goodfellow2014explaining} found that a wide variety of models with different architectures misclassify the same adversarial example, even when trained on different subsets of training data. Those examples are called transferable adversarial examples. These findings indicate that adversarial examples pose a fundamental blind spot in machine learning algorithms.

\subsubsection{Adversarial Examples in CV}
In research, we distinguish between two types of settings.
An attacker operates either in a black or white-box setting, depending on the degree of access, he or she has to the system. While in a black-box setting, the attacker does not have access to the target model's internal architecture or its parameters, he or she has full access to the target model, its parameters, and input feature representations in the white-box setting. The adversary cannot alter the training data or the target model itself in both cases.

Other works have designed attacks in the "extended white-box" or "gray-box" setting, which describes the scenario of some aspects of the setup being known to the attacker while some are not \cite{vivek2018gray}. However, the definitions for those scenarios seem to vary in the different approaches.

Depending on the purpose of the adversarial attacks, they can be categorized as targeted and non-targeted attacks. While in a targeted attack, the output category of a generated example is intentionally controlled to a specific target category, a non-targeted attack does not take the category of misclassified results into consideration \cite{vijayaraghavan2019generating}.


%-------------------------------------------------------------------
%--------------Adversarial Examples in TEXT-----------------------
%------------------------------------------------------------------
    \label{sec:adversarial_exapmles_in_text}

In this thesis, we will further go into the exploratory approach, and, in the following chapter compare adversarial attacks in computer vision with those in NLP and study different approaches proposed so far.

\subsubsection{Differences in Adversarial Attacks for CV and NLP }
    \label{sec:differences_in_adv-ex}

Using gradient-based adversarial methods, as used in computer vision, for attacks in the text domain, can result in altered semantics, syntactically-incorrect sentences, or invalid words that cannot be matched with any words in the word embedding space  \cite{zhang2019adversarial}. When attacking textual DNNs, it is crucial to carefully design variants or distance measures of the perturbation.

There are three main differences in attacking DNN models for computer vision and natural language processing:

\textbf{1. Discrete vs. continuous inputs}

Typical methods to generate adversarial examples in the image domain use $L_p$ norms to measure the distance between an original data point and a perturbed data one. 
In an image, each pixel has a number representation within a fixed range.  A common representation is to use floating numbers in the range ${[0,1]}$ or ${\{0,1,}$\dots${,255\}}$. This indicates that the numerical representation of a pixel gives an insight into the characteristics of the image, and from this, it can be deduced that pixels with similar numerical representations are closely related in terms of their characteristics. The case is different with the numerical representation of word tokens.
Since textual data is symbolic, thus discrete, it is not possible to adapt this in the text-domain, since adopting this method from computer vision often results in altered semantic meaning or syntactically-incorrect sentences. The generation of adversarial examples in the text-domain is therefore considered more challenging\cite{carlini2018audio} 

\textbf{2. Semantic meaning vs. no semantic meaning}

When generating adversarial images, it is very unlikely to change the picture's semantic meaning. This is different in the text domain, where a change of a sentence's semantic meaning happens easily. As described in chapter~\ref{sec:word_embeddings}, the mapping from natural language into a corresponding vector is done through the generation of word embeddings.
Given the example sentence, "I like cats", both, the generation of an adversarial example on word-level (e.g., changed word: $cat$ - generated adversarial sentence through word-swap:  "I like dogs"), as well as on character-level (e.g., changed character: $t$ - generated adversarial example through character-swap: "I like cars"). Thus, the numerical representation of the word 'cat' might be close to that of the word "dog".
However, that does not mean that the semantic meaning of the words are similar, as it is the case with pixels.


\textbf{3. Perceptible vs unperceivable}

Small modifications of image pixels are hardly recognized by human beings, as shown in Figure ~\ref{fig:advEx}. Hence, if successful, adversarial examples will change the DNNs prediction but not human judgment. Small changes in the text, e.g., character or word modifications, however, are easily detected and, therefore, render the possibility of attack failure. Moreover, a modification can be identified not only by humans but might also be corrected by spelling- or grammar-check systems. Hence, it is not trivial to find imperceivable textual adversarial examples.



\subsection{Adversarial Attacks in NLP}
    \label{sec:adv-ex_in_NLP}
    
In recent years the research motivation to generate adversarial examples NLP has increased. Jia and Liang were the first to conduct adversarial attacks on textual DNNs and gained attention in the NLP community\cite{Jia2017AdversarialEF}. They address the reading comprahension task and showed that they could cause models to ouptput wron answeres by appending distractor sentences to the end of their input sequences. Inspired by their work, Glockner et al. conducted attacks using the entailment task on the word-level by replacing one word with either a synonym or its hypernym \cite{glockner2018breaking}. Since then many studies have investigated the security of NLP task DNNs and proposed several attack methods. Many papers on the topic distinguish between causative and exploratory attacks. Ling Huang et al., define the terms as follows:

Causative -  Causative attacks alter the training process through influence over the training data.

Exploratory - Exploratory attacks do not alter the training process but use other techniques, such as probing the detector, to discover information about it or its training data.
\cite{huang2011adversarial}

To better compare the numerous different attack methods suggested for NLP models so far, we will use a unified definition framework provided by Morris et al., introduced in their paper on the Software Framework TextAttack for adversarial attacks in NLP\cite{morris2020textattack}. They determine four components, namely a goal function, a search method, a transformation, and an optional list of constraints. In the following chapter, we will explain those components in more detail and assemble attacks from the literature, by means of the components.


\subsubsection{Goal Function}
\label{sec:goal_funciton}
The goal function determines if an attack is finished, given an input $x$. 
The applied parameters are described in the previous sections and include e.g., the attacker's model knowledge and the target class.
An example would be the goal function:

$G(x):= \{argmax$F(x)$ \neq $original\textunderscore class$\}$

for the attack being successful if the prediction after the attack is unequal the original prediction. 

Proposed goal functions in the literature include untargeted \cite{ebrahimi2017hotflip, alzantot2018generating, li2018textbugger, jin2019bert, garg2020bae}, and mixed goal functions \cite{gao2018black} in the white-box setting \cite{papernot2016crafting, liang2017deep, ebrahimi2017hotflip, jin2019bert} and the black box setting \cite{gao2018black, goodman2020fastwordbug}.

\subsubsection{Search Method}
\label{sec:search_method}

A search method's aim is to find a perturbation that generates a successful adversarial example and satisfies the constraints. There have been many different approaches introduced for this process over the years. The most popular can be categorized into:

    \textbf{a) Greedy with Word Importance Ranking}
    
        The Greedy approach ranks all words of the input according to some ranking system. The perturbation is then conducted either on the most important word or one at a time in order of decreasing importance \cite{li2018textbugger, jin2019bert, garg2020bae}.
    
        Liang et al. were the first to enhance their attacks' efficiency by introducing a two-step approach to generate adversarial attacks in text. They first determine the text items essential for their classification by computing the cost gradient of the input (white-box), as well as generating a series of occluded test samples (black-box attack)\cite{liang2017deep}. 
        Since the adversary in a black-box setting does not have any indication of where the model could be prune to attacks, a two-step approach is required to enhance the efficiency by not having to try all possible modifications or to attack randomly chosen words. 
        In their paper, Gao et al. used the two-step approach. They first determine the words critical for the model's prediction by introducing a word importance scoring function. In a second step, they generate adversarial text by making imperceptible edit operations to the word previously determined 'important' words, forcing the classifier to make wrong predictions \cite{gao2018black}.
        In 2019, the pre-trained BERT model was attacked by adversarial examples for the first time by Jin et al. They conducted a word importance ranking by analyzing the prediction change before and after deleting one word, also called Leave-One-Out Method.  
        
        \textbf{Leave One Out Method for Important-Word-Detection}
        

        Leave One Out is an interpretation method used to determine the words of a document that contribute most to the output of a model prediction. The effort to create a successful adversarial example can be minimized by targeting the attack on the word with the strongest influence on the prediction outcome. The leave one out method resembles a process that iteratively removes one word after another and predicts each sentence without the removed word. 
        This way, a document consisting of e.g., ten words, is sent through the Machine Learning Model eleven times. (Once the complete sentence is predicted, then without the first word, without the second word, and so on). The words which changed the model's prediction of the sentence, through their absence, are considered important words. 
        
    \textbf{b) Beam Search}
    
        Ebrahimi et al., introduced HotFlip, a method to generate adversarial examples with character substitutions, using the gradient with respect to a one-hot input representation, aiming to determine the individual change with the highest estimated loss. They use the beam search to find a set of manipulations that fool the classifier best when conducted together \cite{ebrahimi2017hotflip}. The Beam Search initially scores transformations at all positions in the input and ascertain the top b transformation (where b is a hyperparameter known as the "beam width"). After that, it iterates, looking at transformations of all sequences in the beam \cite{tillmann2003word}. 
        
    \textbf{c) Genethic Algorithm}
    
        Alzatot et al. were the first to use the genetic algorithm as a search method. Instead of relying on gradient-based optimization, they developed an attack algorithm that exploits population-based gradient-free optimization through genetic algorithms. Genetic algorithms are inspired by the process of natural selection, iteratively evolving a population towards better solutions. They determine the best solution through a combination of crossover and mutation. That way, they can find successful adversarial examples with fewer modifications \cite{alzantot2018generating}.
        
        
\subsubsection{Transformation}
\label{sec:transformation}
    The transformation takes an input and returns all potential perturbations while being agnostic of goal function and constraints. Transformation methods can be categorized according to the attacker's model knowledge (white-box and black-box).

    Papernot et al. were the first in 2016 to generate adversarial examples in text. They investigated on adversarial inputs for RNNs processing sequential data using the Fast Gradient Sign Method (FGSM) to generate adversarial sequences in the white-box setting \cite{papernot2016crafting}. 
    Gradient calculations are used to determine which features have to be modified to generate an adversarial example. The method is very effective for generating adversarial examples in the field of computer vision \cite{goodfellow2014explaining}.
    Papernot et al. found that switching from computer vision to natural language processing applications using the FGSM introduced difficulties (as explained in the previous chapter). 
    Goodman et al. generated small utility-preserving text perturbations in a black-box setting by using a scoring method to identify important words that affect text classification \cite{goodman2020fastwordbug}, similar to Gao et al., who, in 2018, use a scoring strategy to generate character-level transformations in the black-box setting on RNN based natural language classifiers. They conduct small edit operations to a text sequence such that a human would consider it similar to the original sequence. They do this by first targeting the important tokens in the sequence and then execute a modification on those tokens that can force the classifier to make a wrong prediction. They perturb using four methods, namely substitution of a letter in a word with a random letter, deletion of a random letter, insertion of a random letter, and swapping two adjacent letters. Their method achieves better results than FGSM  and finds that the selection of words to perturb is more important than the alteration of words\cite{gao2018black}.  In 2017 Liang et al. were able to successfully fool character- and word-level DNN-based text classifiers through the conduction of three perturbation strategies, namely insertion, modification, and removal \cite{liang2017deep}. In 2019, Jin et al. were the first to attack the pre-trained BERT model. Their perturbation approach is word-level based, as they replace the original word with synonyms and use word embedding vectors to measure how well different models judge the semantic similarity between the words \cite{jin2019bert}.
    
    
\subsubsection{Constraints}
\label{sec:constraints}
    When conducting modifications on a textual input sequence of an NLP model, it is likely to alter the semantic meaning or grammatical correctness. E.g.,  the FGSM method to generate modified input. There is no guarantee that words close in the embedding space are semantically similar. Therefore, this approach may replace words with semantically different others and might alter the whole sentence's semantics.
    
    An attack is considered valid if the input satisfies each of the attacker's constraints. 
    Constraints include different approaches. One approach is to measure the edit distances, e.g., the maximum percentage of words changed \cite{ebrahimi2017hotflip} or maximum Levenshtein edit distance \cite{gao2018black}. Samanta et al. use saliency maps to ensure semantically meaningful adversarial examples\cite{samanta2017towards}. This approach, however, is difficult to perform automatically. Grammatically constraints are intended to prevent the attack from creating perturbations that introduce grammatical errors. Examples here are Part-of-speech consistency or a maximum number of grammatical errors \cite{ebrahimi2017hotflip, jin2019bert}. Semantics constraints aim to preserve the meaning between $x$ and $x\textunderscore adv$, e.g., maximum swapped word embedding distances, or using sentence encoders or language models \cite{cer2018universal, garg2020bae, jin2019bert, li2018textbugger, alzantot2018generating, ebrahimi2017hotflip}.


%-------------------------------------------------------------------
%-------------------------------------------------------------------
%-----------------------------Methodology--------------------------
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\section{Methodology}
\label{sec:methodology}

- reference to theory, what the problem is
- why it is a problem

Building on the theoretical foundations described in previous chapters, we will now explain how we proceeded in the practical part of our work. 

\subsection{Objectives todo}
\label{sec:objectives}
- what objectives
- rq

In our thesis we wanted to determine the robustness of the famous BERT model and, hence find out if it can be fooled by minor changes. The aim of the work was answer the following research question:

RQ1: How robust is BERT based NLP model for the task of ABSA against input level adversarial examples?

Our experiments will result in datasets containing adversarial examples.

Later on, we will try to generate examples by modifying these words.
The prediction is considered changed as soon as one of the three output categories (entity, attribute, or sentiment) is changed by omitting a word. ]

%-------------------------------------------------------------------
%-------------------------Setup -----------------------------
%------------------------------------------------------------------

\subsection{Experimental Setup} 
To answer our research question we conducted three input character-level adversarial attacks against the prominent pre-trained BERT model introduced by Google in 2018. 
We evaluated the robustness of our model in an in-domain classification task, called ABSA. As described before, the ABSA task refers to the prediction of an aspect mentioned in an input sequence and the corresponding sentiment. ABSA is considered a multilabel task, since $[0,1, \dots n]$ labels can be predicted for one input sequence. 

To better illustrate our proceedings, we describe it by using the proposed framework by Morris et al,:

Our \textbf{goal function} is represented by the following formula: ${f( x_adv ) = y_adv \neq y}$. It is described in more detail in section ~\ref{sec:experiments}

The \textbf{Search Method} used in this thesis is called Leave-One-Out (LOO) and discussed in section ~\ref{sec:iwd_method}.

\textbf{Transformations} are conducted on the character-level rather than the word-level and elaborated on in section ~\ref{sec:attacks_method}.

To determine \textbf{Constraints} and how our attacks are evaluated, please see section ~\ref{sec:evaluation_tb}.

\subsubsection{Attacks}
\label{sec:attacks_method}
As most existing approaches generate adversarial examples as attacks by finding the minimum perturbation \cite{liang2017deep, ebrahimi2017hotflip, gao2018black, li2018textbugger, alzantot2018generating, jin2019bert, garg2020bae}, the attacks tend to be conspicuous and can easily be identified by humans. Moreover, they are not likely to appear in nature, making them less relevant in practical applications. Perturbations are either word or character level based. Possible variations have been proposed in the literature and are described in chapter ~\ref{sec:adversarial_exapmles_in_text}. Examples are randomly replacing, deleting, swapping, or inserting characters. Perturbations on the word level basis use the same example approaches.  
The disadvantage of word-level perturbations is that an unintentional change of the semantic meaning of a sentence is likely to happen. This means that the comparison of a sentence's prediction before and after the change of a word is no longer guaranteed, which poses a constraint. Various methods have been proposed to avoid this problem and are described in section \textbf{Constraints} in chapter ~\ref{sec:adversarial_exapmles_in_text}. 
This phenomenon occurs less often when perturbing on the character level since the perturbed words remain the same. An exception is when the change of single characters randomly results in a new, existing word of the dictionary with a different semantic meaning. 
To prevent humans from easily identifying our attacks as adversarial examples, we propose three character-level attack methods that are likely to occur in a practical scenario. The attacks can be subdivided into three attack methods:


\textbf{Leet speak}

Leet speak (or '1337') is a system of modifying spelling, primarily used in online games, internet platforms as well as in the 'hacker'-community. The term 'leet' derives from the word 'elite' and describes the communities that use it. The purpose of using leet speak can be to exclude newcomers (e.g., in an online community or game) or outsiders in general. The goal is to create language, that the average reader cannot make sense of \cite{thomas2002hacker}.   
Leet speak is characterized by the use of non-alphabet characters to substitute one or multiple letters of one word with visually similar-looking symbols, so-called homoglyphs. Commonly used homoglyphs in leet speak are numbers. 
We conducted experiments to explore the effect of leet speak on the BERT model. As described in chapter~\ref{sec:the_history_of_NLP} DNN-based text classification continuously gains importance for enhancing the safety of online communication e.g., in online discussion environments through toxic content detection and other methods \cite{kumar2011natural}. This are the environments where leet speak is used primarily. The investigation of what effects the character substitution has on the used BERT models could help to increase security in such forms. 

\textbf{Spelling mistakes}

As described, the BERT base model is pre-trained on Wikipedia and book corpus \cite{zhu2015aligning}, a dataset containing over 10,000 books of different genres. Since it is used for different use cases, of which many include the processing of user-generated data, the BERT model should be robust against spelling mistakes. Sun et al., were the first in 2020 to study the effect of spelling mistakes on the BERT model using different methods \cite{sun2020adv}. Just like them, we will make use of the list of common misspellings generated by Wikipedia\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings}} to generate adversarial examples. The difference to their work is the method for identifying the word to execute the perturbation on. 
The words in the list are gathered by users reporting misspellings. A misspelling is considered common and added to the list if it occurs in Wikipedia at least once a year.
The list consists of 4282 entries, where one word can have multiple misspelling variations. 

\textbf{Punctuation}

To the best of our knowledge, there have not been any investigations on the effect of changed punctuation symbols on the robustness of a BERT model. Since we were surprisingly able to generate successful attacks by merely adding one punctuation mark, we limited ourselves to that and did no further research on different punctuation variations. Moreover, one additional comma is unobtrusive, occurs in the practical use cases, and is not easily identified as an adversarial example by a human observer.


\subsubsection{Target Model} 
\label{sec:target_model_method}

As the base for our experiments we used the uncased BERTbase model\footnote{https://huggingface.co/bert-base-uncased} and in a first step, pre-trained it on the laptop domain and in a second step, fine-tuned in on the ABSA task. In pre-training, BERT solves the two calssification tasks MLM and NSP usually on a large corpora of unlabeled text. We use Amazon Laptop Reviews scraped by He et. al. \cite{he2016ups}. We used the Adam optimizer and 32 bit floating points computations. Adam is an optimization method used to update network weights iteratively in the training process of deep learning models. This optimizer, being a replacement for the stochastic gradient descent, is commonly used since it comes with a straight-forward configuration process and default parameters are suitable for most tasks \cite{kingma2014adam}. We pre-train our model using a batch size of 32 and set the learning rate to 3 $*$ 10$^{-5}$.
We define a maximum input sequence length of 256 tokens, resulting in four sentences per sequence on average.  Due to the relatively low number of training data, we pre-train BERTbase for 30 epochs, so that the model sees about 30 million sentences during training. That way, a single sentence appears multiple times within the two language model tasks. Please find the code for pre-training on GitHub, to reproduce the langauge model\footnote{https://github.com/deepopinion/
domain-adapted-atsc}. 

For fine-tuning the pre-trained BERT model on the ABSA task, we, again, used 32 bit floating point computations, the Adam optimizer and set the learning rate to 3 $*$ 10$^{-5}$. We used a batch size of 32 and fine-tuned it for 20 epochs and random initialization. While the training loss improved continuously, validation accuracy converges already after the xxth epoch of training. Our experiments are based on the model at the xth ... 

\subsubsection{Important Word Detection}
\label{sec:iwd_method}
As mentioned in section ~\ref{sec:adversarial_exapmles_in_text}, the two-step approach to conduct adversarial examples in the text domain is an effective method commonly used in research. The two steps are: firstly, determining the words which have a critical influence on the model's prediction and secondly, perturbing the identified important words. This approach allows for effective fooling of the target classifiers and minimizing the input modification effort \cite{jin2019bert}. Several methods to determine the important word have been proposed in literature.
In 2018 Feng et al. introduced input reduction, a method to enhance the interpretation of neural models \cite{feng2018pathologies}. They aim to find a subset of the most important words of an input sequence that contributes most to a prediction by resembling a process that iteratively removes 'unimportant' words for the input until the prediction of the model changes. The remaining set of words before the predictions changes are considered critical for the prediction output. To decide on which words to remove, they compare the prediction probability towards the original label of a sentence containing the word with the prediction probability of a sentence where the same word was removed. This method is called Leave One Out (LOO) and can itself be used as an approach to determine the important word of an input sequence. The LOO method was introduced by Vehtari et al. to estimate pointwise out-of-sample prediction accuracy \cite{vehtari2017practical}. 

% Important word detection
Given an input sentence $\textbf{x}$ represented as a list of word tokens ${\textbf{x} = [x_1, x_2,}$ \dots${x_n]}$ and a predicted output label $y$. The important word is defined as 
.....
\todo{formula}

In different terms, the importance of a word is determined by comparing the output label $y$ with the ...

This formula is applied for every word token $x$ in a input sequence to determine all words influential to the prediction. Therefore, one input sequence can contain $[0,1, \dots n]$ important words. 
Comparing the prediction before and after a word is removed reflects how the word influences the classification result. This procedure allows for enhanced efficiency of adversarial attacks in the text domain.


\subsubsection{Datasets} 
\label{sec:datasets}
We conduct our experiment using SemEval-2015 Task 12: Aspect Based Sentiment Analysis dataset for the laptops domain \cite{pontiki2015semeval}. An aspect category is defined as a conbination of an entity type $E$ and an attribute type $A$. E describes the reviewed entity (e.g. laptop), while A describes a particular attribute of E (e.g. durability or quality).
To give an example:
Given the review \textit{"They sent it back with a huge crack in it and it still didn't work; and that was the fourth time I've sent it to them to get fixed"}, E could be \textit{customer support} and A could be \textit{quality}. The third slot is used to determine the sentiment polarity towards the entity and its attribute (\textit{positive, neutral or negative}). The sentiment for the above given example would be \textit{negative}.
The three slots are linked to each other within tuples. For our thesis we decided to use only one of the three provided domains, namely Laptops. In deciding why Laptops were chosen instead of Restaurants or Hotels, there is no specific reason other than the researchers' personal interest in laptops. The Laptop dataset contains 1,739 sentences in the training data and 761 sentences in the test data set. Figure ~\ref{fig:semeval} provides a table of entity and attributes labels, used in the dataset.

\begin{figure}
    \centering
    \includegraphics[ width = .5\textwidth]{img/semeval.png}
    \caption{Laptop Entity Aspect Categories  \cite{pontiki2015semeval}.}
    \label{fig:semeval}
\end{figure}

To pre-train the BERT language model on the laptop domain, we use Amazon Laptop reviews \cite{he2016ups}. To avoid training bias for the SemEval-2015 test data, we filtered out reviews that appeared in both, the Amazon and the SemEval Test Dataset. Moreover, we removed reviews that contain less than two sentences from the training corpora, to achieve compatibility with the NSP task used for fine tuning. After the text pre-processing there are 1,007,209 sentences left in the corpus. 



\subsection{Evaluation todo!!}
    \label{sec:evaluation_tb}
As described in chapter ~\ref{sec:adversarial_exapmles_in_text} there are a number of methods proposed to generate adversarial examples in the text domain. Since it is not trivial to assess the quality of these different attacks, as small perturbations can lead to significant changes in semantics meaning of the textual input. In their paper, Xu et al. propose Elephant in the Room, an evaluation framework to assess the quality of adversarial examples in the text domain\cite{xu2020elephant}. Their (automatic) evaluation criteria are:
(a) attacking performance (i.e. how well they fool the classifier); 
(b) textual similarity between the original input and the adversarial input; and
(c) fluency of the adversarial example.

More approaches on the evaluation of adversarial attacks in the text domain are explained in section ~\ref{sec:constraints}. In order to guarantee the constraints, more or less effort must be made, depending on the evaluation method. Some semantic, and grammatical evaluation methods can be performed using additional DNN-based classifiers \cite{ebrahimi2017hotflip, jin2019bert, garg2020bae}.

Other constraints, e.g. to ensure remaining semantic meaning, or to preserve context fitting, can be evaluated by humans only. 
Common ways to evaluate the effectiveness of adversarial attacks are the determination of F-Scores and accuracy for whole datasets, or confidence levels for single predictions. 


\section{Experiments todo}
\label{sec:experiments}

The experiments in this thesis are conducted in the black-box setting, as there is no knowledge about the model architecture or the training data needed. Since there is more practical relevance for this scenario, we will conduct non-targeted attacks and focus on the input level rather than the embedding or semantic level to execute our perturbations. 

To limit the scope of this work, we refrain from reviewing the semantic meaning of the generated adversarial sentences.
As described, adversarial attacks in computer vision differ from the ones in the text domain and are therefor considered more challenging. We adapted the two-step approach introduced in the literature and were able to enhance efficiency. 
....

\subsection{Goal Function}
 \label{sec:goal_function_m}
We have the pre-trained BERTbase language model $f$: ${X \rightarrow Y}$ for the task of ABSA, where $X$ are the input text sequences, and $Y$ are the corresponding labels consisting of E entity, A attribute, and S sentiment. We generate adversarial examples ${x_adv}$ from the original input $x \in X$, and the original prediction $y \in Y$. We do this by determining the important word $i$ of a sequence and replacing it the modified word $i_adv$. Since we aim for changing the model's prediction, our goal function is the following: 
$f$( $x_adv$ ) = $y_adv$ \neq $y$.


To conduct the experiments we used the described SemEval 2015 dataset conatining 1971 reviews on laptops. Before working with the data, we converted it in an adequate format and filtered for unique sequences, resulting in a dataset of 1396 items in the following format:

\texttt{['This computer is absolutely AMAZING!!!', '10 plus hours of battery...']}


\subsection{Search Method}
\label{sec:search_method_m}
Since we operate in the black box setting and therefore do not have access to prediction probability values, as needed for the input reduction method, we make use of the LOO-method to determine the words of a sequence critical for the output of the model's prediction. That way can ascertain which word to place our attack on, in order to achieve maximal efficiency. The theoretical basics of the method are discussed in section ~\ref{sec:iwd_method}.
First, we predicted the 1396 original examples and stored the labels. After that we tokenized the examples by creating a list for each sequence containing the respective words. In the next step, we iterated over the lists of words in a sequence and removed the respective word in the original sentence. We repeated this step for each word in the original sentence. 
After this process, we have as many modified sentences as we have original times the respective number of words. A word is considered important, if it's omission triggered a flip in the prediction, compared to the original sentence. 
In 943 sequences, we were able to find 1 or more important words. The cumulative number of successful modifications caused by the LOO-method 2694. In other words, one sequence contains 1.93\footnote{2694/1396} important words on average.

\subsection{Transformation}
\label{sec:transformation_m}
In section ~\ref{sec:attacks_method} we proposed three attack methods and discussed the theoretical background. We now elaborate on the technical implementation, carried out to generate adversarial examples.

\textbf{Leet Speak}

To generate leet speak, we take the important words of a sentence and apply the function below. That way we swap the letters a, e, l, o, and s with the numbers 4, 3, 1, 0, and 5 respectively. We limited ourselves to this 5 characters in order to preserve the readability. 
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\begin{lstlisting}
def to_leet(word):
    getchar = lambda c: chars[c] if c in chars else c
    chars = {"a":"4","e":"3","l":"1","o":"0","s":"5"}
    return ''.join(getchar(c) for c in word)
\end{lstlisting}
Of a total of 943 sequences, we were able to modify 897 resulting in 2232 cumulative modified sequences. This means, in 46 sentences, the important words did not contain the letters a, e, l, o, or s, and we therefore could not swap a letter with a number.
After the modification, we predicted the modified sequences and compared the prediction with the prediction of the original sequences. 

\texttt{original\_sentence: "super fast processor and really nice graphics card..",
modified\_sentences: 
"super f45t processor and really nice graphics card..", 
"super fast pr0c3550r and really nice graphics card.."]}

\textbf{Spelling Mistakes}






\textbf{Punctuation}





\subsection{Constraints and Evaluation}
    \label{sec:constraints_m}
~~~vage~~

We evaluate our experiments using two of Xu et al.'s proposed evaluation criteria mentioned in section ~\ref{sec:evaluation_tb}. 

\textbf{Attacking Performance}

We consider a an adversarial example successful, if we were able to switch the prediction of a sentence $s$, resulting in ${f(x_adv ) = y_adv \neq y}$.
We consider ${y_adv \neq y}$ if we were able to change one, or more of the three prediction-parts for a sentence $s$: E entity, A attribute, or S sentiment. 
A prediction can be changed in three different ways:
\begin{itemize}
\item a different E, A or S is predicted,
\item the model does no longer predict a label, or
\item the model predicts a label, when it did not predict one before.
\end{itemize}

\textbf{Textual Similarity between $x$ and $x_adv$}

\textbf{Leet}
\textbf{Typo}
\textbf{Punctuation}


As described in their paper, it involves a great deal of effort to ensure (b) textual similarity between the original and adversarial input and (c) the fluency of the adversarial example.
.. word-level


\subsection{Results todo}



\section{Discussion}

\section{Conclusion, Limitations \& further Research}

\subsection{conclusion}
[expos]
The idea is to implement the experiments in the running BERTbased ABSA Deep Learning Hotel Model of the company DeepOpinion. The model is already fine-tuned on user-generated reviews and trained on the ABSA text classification task. This gives me a head start, which limits the scope of my work. However, the model being the company's core technology, cannot be published in my thesis, which limits the reproducibility of my work.

%-------------------------------------------------------------------
%------------------------------Limitations -----------------------
%-------------------------------------------------------------------

\subsection{Limitations}

- other domains hotels and restaurants

%-------------------------------------------------------------------
%---------------------------Further Research -----------------------
%-------------------------------------------------------------------

\subsection{Further Research}

%END { system('cp', 'output.bbl', 'main.bbl'); }

 \nameref{Methodology}


%%% End:

\FloatBarrier
\bibliography{References}

